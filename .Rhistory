#labelling the matrix data
colnames(lhs.many.values) <- c('location', 'scale', 'likelihood')
#creating empty raster <- still not really sure what this does
empty <- extent(lhs.many.values[,1:2])
next.step5 <- raster(empty, ncol=100, nrow=100)
last.step5 <- rasterize(lhs.many.values[,1:2], next.step5, lhs.many.values[,3], fun=mean)
plot(last.step5, xlab='location', ylab='scale')
title("location v. scale, z axis = likelihood")
#log liklelihood
plot(lhs.cube.update2[seq(from =1, to =1e6, by =1000),1], lhs.all.params2[seq(from =1, to =1e6, by =1000)], ylim=c(-1000,0))
plot(lhs.cube.update2[seq(from =1, to =1e6, by =1000),1], exp(lhs.all.params2[seq(from =1, to =1e6, by =1000)]))
lhs.many.values2 <- cbind(lhs.cube.update2[,2], lhs.cube.update2[,3], lhs.all.params2)
colnames(lhs.many.values2) <- c('scale', 'shape', 'likelihood')
empty2 <- extent(lhs.many.values2[,1:2])
next.step6 <- raster(empty2, ncol=1000, nrow=10)
last.step6 <- rasterize(lhs.many.values2[,1:2], next.step6, lhs.many.values2[,3], fun=mean)
plot(last.step6, xlab='scale', ylab='shape', ylim=c(0,50), xlim=c(1000,2000) )
title("scale v. shape, z axis = likelihood")
lhs.many.values3 <- cbind(lhs.cube.update2[,1], lhs.cube.update2[,3], lhs.all.params2)
colnames(lhs.many.values3) <- c('scale', 'shape', 'likelihood')
empty3 <- extent(lhs.many.values3[,1:2])
next.step7 <- raster(empty3, ncol=1000, nrow=10)
last.step7 <- rasterize(lhs.many.values3[,1:2], next.step7, lhs.many.values3[,3], fun=mean)
plot(last.step7, xlab='location', ylab='shape', ylim=c(0,10), xlim=c(1400,1450))
title("location v. shape, z axis = likelihood")
#order the likelihood in order to find the most likely values of certain parameters
lhs.sorted <- rev(order(lhs.all.params2))
#taking top 10 percent of all of this, gives us all of the top , these are just the indicies though
lhs.top1p <- round(length(lhs.sorted)*.01)
#this is list of all of the indicies that we need
ind.to.use <- lhs.sorted[1:lhs.top1p]
#plotting location and scale of top 1 percent - no colors yet
plot(lhs.cube.update2[ind.to.use,1], lhs.cube.update2[ind.to.use,2], xlab = 'location', ylab = 'scale')
title('location v. scale')
plot(lhs.cube.update2[ind.to.use,2], lhs.cube.update2[ind.to.use,3], xlab = 'scale', ylab ='shape')
title('scale v. shape')
plot(lhs.cube.update2[ind.to.use,1], lhs.cube.update2[ind.to.use,3], xlab = 'location', ylab = 'scale')
title('location v. shape')
#-------trying to create survival function plot with Latin Hyper Cubes
#first lets find the best fitting parameters of the entire data set - aka the highest likelihood
ind.high.like <- lhs.sorted[1]
colnames(lhs.cube.update2) <- c('location', 'scale', 'shape')
sf.loc1 <- lhs.cube.update2[ind.high.like,1]
sf.scale1 <- lhs.cube.update2[ind.high.like,2]
sf.shape1 <- lhs.cube.update2[ind.high.like,3]
x.hgt <- seq (0,3000, by=10)
#list of indicies of max sea level sorted from lowest to highest
lsl.sort <- order(lsl.max)
lsl.sorted.vals <- lsl.max[lsl.sort]
#with the location, scale and shape that was given to us with the GEV curve parameters way above (around line 200-210)
sf.hgt <- 1-pevd(x.hgt, loc=gev.mle$results$par[1], scale=gev.mle$results$par[2], shape = gev.mle$results$par[3], type=c("GEV"), lower.tail=TRUE)
plot(x.hgt, log10(sf.hgt), type='l')
sf.hgt2 <- 1-pevd(x.hgt, loc=sf.loc1, scale=sf.scale1, shape = sf.shape1, type=c("GEV"), lower.tail=TRUE)
plot(x.hgt, log10(sf.hgt2), type='l')
esf.vals <- seq(from=(length(lsl.sorted.vals)), to=1, by=(-1)) / (length(lsl.sorted.vals)+1)
plot(x.hgt, log10(sf.hgt), type='l', ylab = 'log probability', xlab = 'height [mm] of sea')
title('Survival Function of height of sea [mm], New London CT')
points(lsl.sorted.vals, log10(esf.vals))
lines(x.hgt, log10(sf.hgt2), col='red')
sf.top10 <- lhs.sorted[1:10]
sf.location.top10 <- lhs.cube.update2[sf.top10, 1]
sf.scale.top10 <- lhs.cube.update2[sf.top10, 2]
sf.shapes.top10 <-lhs.cube.update2[sf.top10,3]
#------Top 10 Highest Likelihood Overall
n.lhs.sorted <- length(lhs.sorted)
for (s in 1:10){
sf.hgt.test <- 1-pevd(x.hgt, loc=sf.location.top10[s], scale=sf.scale.top10[s], shape=sf.shapes.top10[s], type=c("GEV"), lower.tail=TRUE)
lines(x.hgt, log10(sf.hgt.test), col='blue')
}
#-------Top 10 Highest Likelihood - Positive Shape Parameter
shape.pos.ind <- lhs.sorted[lhs.cube.update2[lhs.sorted,3] > 0]
shape.pos.ind.top10 <- shape.pos.ind[1:10]
sf.location.top10.pos <- lhs.cube.update2[shape.pos.ind.top10, 1]
sf.scale.top10.pos <- lhs.cube.update2[shape.pos.ind.top10, 2]
sf.shapes.top10.pos <-lhs.cube.update2[shape.pos.ind.top10,3]
for (f in 1:10){
sf.hgt.test2 <- 1-pevd(x.hgt, loc=sf.location.top10.pos [f], scale=sf.scale.top10.pos [f], shape=sf.shapes.top10.pos [f], type=c("GEV"), lower.tail=TRUE)
lines(x.hgt, log10(sf.hgt.test2), col='red')
}
plot(lhs.cube.update2[ind.to.use,1], lhs.cube.update2[ind.to.use,2], xlab = 'location', ylab = 'scale')
title('location v. scale')
plot(lhs.cube.update2[ind.to.use,2], lhs.cube.update2[ind.to.use,3], xlab = 'scale', ylab ='shape')
ind.to.use <- lhs.sorted[1:lhs.top1p]
plot(lhs.cube.update2[ind.to.use,1], lhs.cube.update2[ind.to.use,2], xlab = 'location', ylab = 'scale')
title('location v. scale')
plot(lhs.cube.update2[ind.to.use,2], lhs.cube.update2[ind.to.use,3], xlab = 'scale', ylab ='shape')
title('scale v. shape')
lhs.many.values <- cbind(lhs.cube.update2[,1], lhs.cube.update2[,2], lhs.all.params2)
colnames(lhs.many.values) <- c('location', 'scale', 'likelihood')
empty <- extent(lhs.many.values[,1:2])
next.step5 <- raster(empty, ncol=100, nrow=100)
last.step5 <- rasterize(lhs.many.values[,1:2], next.step5, lhs.many.values[,3], fun=mean)
plot(last.step5, xlab='location', ylab='scale')
title("location v. scale, z axis = likelihood")
plot(lhs.cube.update2[seq(from =1, to =1e6, by =1000),1], lhs.all.params2[seq(from =1, to =1e6, by =1000)], ylim=c(-1000,0))
plot(lhs.cube.update2[seq(from =1, to =1e6, by =1000),1], exp(lhs.all.params2[seq(from =1, to =1e6, by =1000)]))
lhs.many.values2 <- cbind(lhs.cube.update2[,2], lhs.cube.update2[,3], lhs.all.params2)
colnames(lhs.many.values2) <- c('scale', 'shape', 'likelihood')
empty2 <- extent(lhs.many.values2[,1:2])
next.step6 <- raster(empty2, ncol=1000, nrow=10)
last.step6 <- rasterize(lhs.many.values2[,1:2], next.step6, lhs.many.values2[,3], fun=mean)
plot(last.step6, xlab='scale', ylab='shape', ylim=c(0,50), xlim=c(1000,2000) )
title("scale v. shape, z axis = likelihood")
lhs.many.values3 <- cbind(lhs.cube.update2[,1], lhs.cube.update2[,3], lhs.all.params2)
colnames(lhs.many.values3) <- c('scale', 'shape', 'likelihood')
empty3 <- extent(lhs.many.values3[,1:2])
next.step7 <- raster(empty3, ncol=1000, nrow=10)
last.step7 <- rasterize(lhs.many.values3[,1:2], next.step7, lhs.many.values3[,3], fun=mean)
plot(last.step7, xlab='location', ylab='shape', ylim=c(0,10), xlim=c(1400,1450))
files.tg <- list.files(path=dat.dir,pattern=filetype)
data <- read.csv(paste(dat.dir,files.tg[1],sep=''), header=TRUE, sep=septype)
if(length(files.tg) > 1) {
for (ff in 2:length(files.tg)) {
data <- rbind(data, read.table(paste(dat.dir,files.tg[ff],sep=''), header = TRUE, sep=septype))
}
}
#===============================================================================
# Storm surge
#===============================================================================
library(extRemes)
library(lubridate)
library(zoo)
# Get tide gauge data and prepare to analyze.
years         <- data$Year
years.unique  <- unique(years)
n.years       <- length(years.unique)
lsl.mean      <- rep(0,length(n.years))
lsl.max       <- rep(0,length(n.years))
data$lsl.norm <- rep(NA,length(years))
#get all of the hours out of data
hours <- data$Hour
#get the unique hours
hours.unique <- unique(hours)
n.hours <- length(hours)
#finding the difference in hours
hour.diff <- diff(hours)
#subtracting one to make it the same length as the differnece in hours
num <- 1
# hours.strange <- list()
for (i in 1:(n.hours-1)){
#checking if difference between hours is not 1 or -23, will print 'index of data pt that has problem', else does nothing
if (hour.diff[i] != 1 & hour.diff[i] != -23){
#the list of all of the weird hour gaps in the data
hours.strange[num] <- i+1
num <- num + 1
}
}
#now to check how big the gap is
days <- data$Day
day.unique <- unique(days)
n.days <- length(days)
#also need to check to see if there is a month gap because that could also pose the same type of problem
months <- data$Month
n.month <- length(months)
#started at 2 so that the data the data stuff would start at 1939 not 1938
for (tt in 2:n.years) {
ind.thisyear <- which(years==years.unique[tt])
lsl.mean[tt] <- mean(data$Sea_Level[ind.thisyear])
data$lsl.norm[ind.thisyear] <- data$Sea_Level[ind.thisyear] - lsl.mean[tt]
lsl.max[tt] <- max(data$lsl.norm[ind.thisyear])
}
dat.dir <- './data/'
filetype <- 'csv'
septype <- ','
today=Sys.Date(); today=format(today,format="%d%b%Y")
filename.projout <- paste('../output_model/BRICK_project-lsl-surge_NewLondon_',today,'.nc',sep='')
filename.lslout  <- paste('../output_model/BRICK_project-lsl_NewLondon_',today,'.csv', sep="")
setwd('~/codes/Klufas_NewLondon/')
files.tg <- list.files(path=dat.dir,pattern=filetype)
data <- read.csv(paste(dat.dir,files.tg[1],sep=''), header=TRUE, sep=septype)
if(length(files.tg) > 1) {
for (ff in 2:length(files.tg)) {
data <- rbind(data, read.table(paste(dat.dir,files.tg[ff],sep=''), header = TRUE, sep=septype))
}
}
library(extRemes)
library(lubridate)
dat.dir <- './data/'
filetype <- 'csv'
septype <- ','
today=Sys.Date(); today=format(today,format="%d%b%Y")
filename.projout <- paste('../output_model/BRICK_project-lsl-surge_NewLondon_',today,'.nc',sep='')
filename.lslout  <- paste('../output_model/BRICK_project-lsl_NewLondon_',today,'.csv', sep="")
setwd('~/codes/Klufas_NewLondon/')
files.tg <- list.files(path=dat.dir,pattern=filetype)
data <- read.csv(paste(dat.dir,files.tg[1],sep=''), header=TRUE, sep=septype)
if(length(files.tg) > 1) {
for (ff in 2:length(files.tg)) {
data <- rbind(data, read.table(paste(dat.dir,files.tg[ff],sep=''), header = TRUE, sep=septype))
}
}
library(extRemes)
library(lubridate)
library(zoo)
years         <- data$Year
years.unique  <- unique(years)
n.years       <- length(years.unique)
lsl.mean      <- rep(0,length(n.years))
lsl.max       <- rep(0,length(n.years))
data$lsl.norm <- rep(NA,length(years))
hours <- data$Hour
hours.unique <- unique(hours)
n.hours <- length(hours)
dat.dir <- './data/'
filetype <- 'csv'
septype <- ','
today=Sys.Date(); today=format(today,format="%d%b%Y")
filename.projout <- paste('../output_model/BRICK_project-lsl-surge_NewLondon_',today,'.nc',sep='')
filename.lslout  <- paste('../output_model/BRICK_project-lsl_NewLondon_',today,'.csv', sep="")
setwd('~/codes/Klufas_NewLondon/')
files.tg <- list.files(path=dat.dir,pattern=filetype)
data <- read.csv(paste(dat.dir,files.tg[1],sep=''), header=TRUE, sep=septype)
if(length(files.tg) > 1) {
for (ff in 2:length(files.tg)) {
data <- rbind(data, read.table(paste(dat.dir,files.tg[ff],sep=''), header = TRUE, sep=septype))
}
}
library(extRemes)
library(lubridate)
library(zoo)
years         <- data$Year
years.unique  <- unique(years)
n.years       <- length(years.unique)
lsl.mean      <- rep(0,length(n.years))
lsl.max       <- rep(0,length(n.years))
data$lsl.norm <- rep(NA,length(years))
hours <- data$Hour
hours.unique <- unique(hours)
n.hours <- length(hours)
hour.diff <- diff(hours)
num <- 1
for (i in 1:(n.hours-1)){
#checking if difference between hours is not 1 or -23, will print 'index of data pt that has problem', else does nothing
if (hour.diff[i] != 1 & hour.diff[i] != -23){
#the list of all of the weird hour gaps in the data
hours.strange[num] <- i+1
num <- num + 1
}
}
days <- data$Day
day.unique <- unique(days)
for (tt in 2:n.years) {
ind.thisyear <- which(years==years.unique[tt])
lsl.mean[tt] <- mean(data$Sea_Level[ind.thisyear])
data$lsl.norm[ind.thisyear] <- data$Sea_Level[ind.thisyear] - lsl.mean[tt]
lsl.max[tt] <- max(data$lsl.norm[ind.thisyear])
}
fit <- lm(lsl.mean ~ years.unique) #linear model
gev.mle <- fevd(coredata(lsl.max), type='GEV') # extRemes
gev.mle2 <- gev.fit(coredata(lsl.max), show = FALSE)
print(gev.mle$results$par) #printing location, scale, shape for hist
x.lsl <- seq(from=0, to=3000, by=1)
hist(lsl.max, freq=FALSE)
library(fExtremes)
library(extRemes)
library(lubridate)
library(zoo)
years         <- data$Year
years.unique  <- unique(years)
n.years       <- length(years.unique)
lsl.mean      <- rep(0,length(n.years))
lsl.max       <- rep(0,length(n.years))
data$lsl.norm <- rep(NA,length(years))
hours <- data$Hour
hours.unique <- unique(hours)
n.hours <- length(hours)
hour.diff <- diff(hours)
num <- 1
for (i in 1:(n.hours-1)){
#checking if difference between hours is not 1 or -23, will print 'index of data pt that has problem', else does nothing
if (hour.diff[i] != 1 & hour.diff[i] != -23){
#the list of all of the weird hour gaps in the data
hours.strange[num] <- i+1
num <- num + 1
}
}
days <- data$Day
data$lsl.norm <- rep(NA,length(years))
for (tt in 2:n.years) {
ind.thisyear <- which(years==years.unique[tt])
lsl.mean[tt] <- mean(data$Sea_Level[ind.thisyear])
data$lsl.norm[ind.thisyear] <- data$Sea_Level[ind.thisyear] - lsl.mean[tt]
lsl.max[tt] <- max(data$lsl.norm[ind.thisyear])
}
fit <- lm(lsl.mean ~ years.unique) #linear model
gev.mle <- fevd(coredata(lsl.max), type='GEV') # extRemes
gev.mle2 <- gev.fit(coredata(lsl.max), show = FALSE)
print(gev.mle$results$par) #printing location, scale, shape for hist
library(ismev)
gev.mle <- fevd(coredata(lsl.max), type='GEV') # extRemes
gev.mle2 <- gev.fit(coredata(lsl.max), show = FALSE)
print(gev.mle$results$par) #printing location, scale, shape for hist
x.lsl <- seq(from=0, to=3000, by=1)
hist(lsl.max, freq=FALSE)
curve <- devd(x.lsl, loc=gev.mle$results$par[1], scale=gev.mle$results$par[2], shape=gev.mle$results$par[3], threshold=0, log=FALSE, type=c("GEV"))
lines(curve, col = 'red')
loc.move <- seq(from=0, to=3000, by=1)
n.loc.move <- length(loc.move)
curve.loc <- rep(0, n.loc.move)
for (i in 1:n.loc.move){
curve.loc[i] <- sum(devd(lsl.max, loc=loc.move[i], scale=gev.mle$results$par[2], shape=gev.mle$results$par[3], log=FALSE, type=c("GEV")))
}
plot(curve.loc)
curve.scale <- rep(0, n.loc.move)
for (i in 1:n.loc.move){
curve.scale[i] <- sum(devd(lsl.max, loc=gev.mle$results$par[1], scale=loc.move[i], shape=gev.mle$results$par[3], log=FALSE, type=c("GEV")))
}
plot(curve.scale)
shape.move <- seq(from=-2, to=2, by=.02)
n.shape.move <- length(shape.move)
curve.shape <- rep(0, n.shape.move)
for (i in 1:n.shape.move){
curve.shape[i] <- prod(devd(lsl.max, loc=gev.mle$results$par[1], scale=gev.mle$results$par[2], shape=shape.move[i], log=FALSE, type=c("GEV")))
}
plot(shape.move, log(curve.shape))
library(lhs)
library(raster)
lhs.cube2 <- randomLHS(n=1000000,k=3)
lhs.cube.update2 <- cbind(lhs.cube2[,1]*3000, lhs.cube2[,2]*3000, lhs.cube2[,3]*(10) - 5)
lhs.all.params2 <- rep(0,1000000)
niter <-1000000
pb <- txtProgressBar(min=0,max=niter,initial=0,style=3)
for (q in 1:niter){
lhs.all.params2[q] <- sum(devd(lsl.max, loc=lhs.cube.update2[q,1], scale=lhs.cube.update2[q,2], shape=lhs.cube.update2[q,3], log=TRUE, type=c("GEV")))
setTxtProgressBar(pb, q)
}
close(pb)
lhs.many.values <- cbind(lhs.cube.update2[,1], lhs.cube.update2[,2], lhs.all.params2)
colnames(lhs.many.values) <- c('location', 'scale', 'likelihood')
empty <- extent(lhs.many.values[,1:2])
next.step5 <- raster(empty, ncol=100, nrow=100)
last.step5 <- rasterize(lhs.many.values[,1:2], next.step5, lhs.many.values[,3], fun=mean)
plot(last.step5, xlab='location', ylab='scale')
title("location v. scale, z axis = likelihood")
plot(lhs.cube.update2[seq(from =1, to =1e6, by =1000),1], lhs.all.params2[seq(from =1, to =1e6, by =1000)], ylim=c(-1000,0))
plot(lhs.cube.update2[seq(from =1, to =1e6, by =1000),1], exp(lhs.all.params2[seq(from =1, to =1e6, by =1000)]))
lhs.many.values2 <- cbind(lhs.cube.update2[,2], lhs.cube.update2[,3], lhs.all.params2)
colnames(lhs.many.values2) <- c('scale', 'shape', 'likelihood')
empty2 <- extent(lhs.many.values2[,1:2])
next.step6 <- raster(empty2, ncol=1000, nrow=10)
last.step6 <- rasterize(lhs.many.values2[,1:2], next.step6, lhs.many.values2[,3], fun=mean)
plot(last.step6, xlab='scale', ylab='shape', ylim=c(0,50), xlim=c(1000,2000) )
title("scale v. shape, z axis = likelihood")
lhs.many.values3 <- cbind(lhs.cube.update2[,1], lhs.cube.update2[,3], lhs.all.params2)
colnames(lhs.many.values3) <- c('scale', 'shape', 'likelihood')
empty3 <- extent(lhs.many.values3[,1:2])
next.step7 <- raster(empty3, ncol=1000, nrow=10)
last.step7 <- rasterize(lhs.many.values3[,1:2], next.step7, lhs.many.values3[,3], fun=mean)
plot(last.step7, xlab='location', ylab='shape', ylim=c(0,10), xlim=c(1400,1450))
title("location v. shape, z axis = likelihood")
lhs.sorted <- rev(order(lhs.all.params2))
lhs.top1p <- round(length(lhs.sorted)*.01)
ind.to.use <- lhs.sorted[1:lhs.top1p]
plot(lhs.cube.update2[ind.to.use,1], lhs.cube.update2[ind.to.use,2], xlab = 'location', ylab = 'scale')
title('location v. scale')
plot(lhs.cube.update2[ind.to.use,2], lhs.cube.update2[ind.to.use,3], xlab = 'scale', ylab ='shape')
title('scale v. shape')
plot(lhs.cube.update2[ind.to.use,1], lhs.cube.update2[ind.to.use,3], xlab = 'location', ylab = 'scale')
title('location v. shape')
ind.high.like <- lhs.sorted[1]
colnames(lhs.cube.update2) <- c('location', 'scale', 'shape')
sf.loc1 <- lhs.cube.update2[ind.high.like,1]
sf.scale1 <- lhs.cube.update2[ind.high.like,2]
sf.shape1 <- lhs.cube.update2[ind.high.like,3]
x.hgt <- seq (0,3000, by=10)
lsl.sort <- order(lsl.max)
lsl.sorted.vals <- lsl.max[lsl.sort]
sf.hgt <- 1-pevd(x.hgt, loc=gev.mle$results$par[1], scale=gev.mle$results$par[2], shape = gev.mle$results$par[3], type=c("GEV"), lower.tail=TRUE)
plot(x.hgt, log10(sf.hgt), type='l')
sf.hgt2 <- 1-pevd(x.hgt, loc=sf.loc1, scale=sf.scale1, shape = sf.shape1, type=c("GEV"), lower.tail=TRUE)
plot(x.hgt, log10(sf.hgt2), type='l')
esf.vals <- seq(from=(length(lsl.sorted.vals)), to=1, by=(-1)) / (length(lsl.sorted.vals)+1)
plot(x.hgt, log10(sf.hgt), type='l', ylab = 'log probability', xlab = 'height [mm] of sea')
title('Survival Function of height of sea [mm], New London CT')
points(lsl.sorted.vals, log10(esf.vals))
lines(x.hgt, log10(sf.hgt2), col='red')
sf.top10 <- lhs.sorted[1:10]
sf.location.top10 <- lhs.cube.update2[sf.top10, 1]
sf.scale.top10 <- lhs.cube.update2[sf.top10, 2]
sf.shapes.top10 <-lhs.cube.update2[sf.top10,3]
n.lhs.sorted <- length(lhs.sorted)
for (s in 1:10){
sf.hgt.test <- 1-pevd(x.hgt, loc=sf.location.top10[s], scale=sf.scale.top10[s], shape=sf.shapes.top10[s], type=c("GEV"), lower.tail=TRUE)
lines(x.hgt, log10(sf.hgt.test), col='blue')
}
shape.pos.ind <- lhs.sorted[lhs.cube.update2[lhs.sorted,3] > 0]
shape.pos.ind.top10 <- shape.pos.ind[1:10]
sf.location.top10.pos <- lhs.cube.update2[shape.pos.ind.top10, 1]
sf.scale.top10.pos <- lhs.cube.update2[shape.pos.ind.top10, 2]
sf.shapes.top10.pos <-lhs.cube.update2[shape.pos.ind.top10,3]
for (f in 1:10){
sf.hgt.test2 <- 1-pevd(x.hgt, loc=sf.location.top10.pos [f], scale=sf.scale.top10.pos [f], shape=sf.shapes.top10.pos [f], type=c("GEV"), lower.tail=TRUE)
lines(x.hgt, log10(sf.hgt.test2), col='red')
}
library(DEoptim)
log.like.calc <- function(data, location1, scale1, shape1){
ll<-sum(devd(data, loc=location1, scale=scale1, shape=shape1, type=c('GEV'), log=TRUE))
return(ll)
}
neg.log.like.calc <- function(p, data){
mu <- p[1]
sigma <- p[2]
xi <- p[3]
nll <- -1*sum(devd(data, loc=mu, scale=sigma, shape=xi, type=c('GEV'), log=TRUE))
return(nll)
}
upper.bound <- c(3000,3000,5)
lower.bound <- c(0,0,-5)
bounds <- cbind(lower.bound, upper.bound)
p.names<- c('mu', 'sigma', 'xi')
de.optim.val <- DEoptim.control(VTR = -Inf, strategy = 2, bs = FALSE, NP = 100,
itermax = 100, CR = 0.5, F = 0.8, trace = TRUE, initialpop = NULL,
storepopfrom = 101, storepopfreq = 1, p = 0.2, c = 0,
parallelType = 0, cluster = NULL, packages = c(), parVar = c(),
foreachArgs = list())
optim.like <- DEoptim(neg.log.like.calc, lower=lower.bound, upper=upper.bound, control=de.optim.val, data= lsl.max)
value.to.use <- pevd(x.hgt, loc = optim.like$optim$bestmem[1], scale=optim.like$optim$bestmem[2], shape=optim.like$optim$bestmem[3], type=c('GEV'), lower.tail = FALSE)
plot(x.hgt, log10(sf.hgt), type='l', ylab = 'log probability', xlab = 'height [mm] of sea')
title('Survival Function of height of sea [mm], New London CT')
points(lsl.sorted.vals, log10(esf.vals))
lines(x.hgt, log10(sf.hgt2), col='red')
for (s in 1:10){
sf.hgt.test <- 1-pevd(x.hgt, loc=sf.location.top10[s], scale=sf.scale.top10[s], shape=sf.shapes.top10[s], type=c("GEV"), lower.tail=TRUE)
lines(x.hgt, log10(sf.hgt.test), col='blue')
}
for (f in 1:10){
sf.hgt.test2 <- 1-pevd(x.hgt, loc=sf.location.top10.pos [f], scale=sf.scale.top10.pos [f], shape=sf.shapes.top10.pos [f], type=c("GEV"), lower.tail=TRUE)
lines(x.hgt, log10(sf.hgt.test2), col='red')
}
lines(x.hgt, log10(value.to.use), col='green')
temp.data <- read.table('../data/noaa_temperature_1880-2017.csv', header = TRUE, sep=',')
log.like.calc <- function(data, location1, scale1, shape1){
ll<-sum(devd(data, loc=location1, scale=scale1, shape=shape1, type=c('GEV'), log=TRUE))
return(ll)
}
neg.log.like.calc <- function(p, data){
if (length(p) == 3){
mu <- p[1]
sigma <- p[2]
xi <- p[3]
nll <- -1*sum(devd(data, loc=mu, scale=sigma, shape=xi, type=c('GEV'), log=TRUE))
}
else if (length(p) == 4){
mu1 <- p[1]
sigma <- p[2]
xi <- p[3]
mu0 <- p[4]
nll <- 1*sum(devd(data, loc=mu, scale=sigma, shape=xi, type=c('GEV'), log=TRUE))
}
return(nll)
}
temp.data <- read.table('../data/noaa_temperature_1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa_temperature_1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa_temprature_1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa_temp_1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
log.like.calc <- function(data, location1, scale1, shape1){
ll<-sum(devd(data, loc=location1, scale=scale1, shape=shape1, type=c('GEV'), log=TRUE))
return(ll)
}
neg.log.like.calc <- function(p, data){
if (length(p) == 3){
mu <- p[1]
sigma <- p[2]
xi <- p[3]
nll <- -1*sum(devd(data, loc=mu, scale=sigma, shape=xi, type=c('GEV'), log=TRUE))
}
else if (length(p) == 4){
mu1 <- p[1]
sigma <- p[2]
xi <- p[3]
mu0 <- p[4]
nll <- 1*sum(devd(data, loc=mu, scale=sigma, shape=xi, type=c('GEV'), log=TRUE))
}
return(nll)
}
setwd('~/codes/Klufas_NewLondon/')
temp.data <- read.table('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.csv('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
temp.data <- read.table('../noaa-temp-1880-2017.csv', header = TRUE, sep=',')
log.like.calc <- function(data, location1, scale1, shape1){
ll<-sum(devd(data, loc=location1, scale=scale1, shape=shape1, type=c('GEV'), log=TRUE))
return(ll)
}
temp.data <- read.table('noaa-temp-1880-2017.csv', header = TRUE, sep=',')
log.like.calc <- function(data, location1, scale1, shape1){
ll<-sum(devd(data, loc=location1, scale=scale1, shape=shape1, type=c('GEV'), log=TRUE))
return(ll)
}
neg.log.like.calc <- function(p, data){
if (length(p) == 3){
mu <- p[1]
sigma <- p[2]
xi <- p[3]
nll <- -1*sum(devd(data, loc=mu, scale=sigma, shape=xi, type=c('GEV'), log=TRUE))
}
else if (length(p) == 4){
mu1 <- p[1]
sigma <- p[2]
xi <- p[3]
mu0 <- p[4]
nll <- 1*sum(devd(data, loc=mu, scale=sigma, shape=xi, type=c('GEV'), log=TRUE))
}
return(nll)
}
temp.data <- read.table('noaa-temp-1880-2017.csv', header = TRUE, sep=',')
temp.years <- temp.data$Year
temp.values <- temp.data$Value
